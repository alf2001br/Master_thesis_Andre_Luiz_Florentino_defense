\chapter{CONCLUSION AND FUTURE WORK}
\label{chp:conclusion}

This project was undertaken to develop and implement an Environmental Sound Recognition (\gls{esr}) algorithm in an embedded system to be deployed in the C-Bot by EDAG GmbH in the next project milestone in 2025. One of the goals of the C-Bot in 2025 is to achieve advanced functionalities for early warning systems based on the sound detection of emergency vehicles and road users. Since the hardware construction of the C-Bots is not located in Brazil, an alternative approach was adopted using a regular passenger vehicle. The \gls{esr} algorithm was embedded in a Raspberry Pi, along with a separate microphone array to emulate the audio processing capabilities of the vehicle control unit and a lapel microphone.

The existing literature on \gls{esr} algorithms implemented in embedded devices for autonomous or regular passenger vehicles is limited, with most studies focusing exclusively on emergency vehicle detection and only a few performing real-time inferences. To achieve the general objective of this project, a dataset benchmarking study was conducted to confirm the accuracy of implemented classifiers. A new dataset was constructed and published in the Harvard Dataverse (US8K\_AV), tailoring environmental sounds to the context of autonomous vehicles for smart cities, namely dog\_bark, children\_playing, car\_horn, siren, silence, and background. Several classifiers were trained and evaluated both on a notebook and on a Raspberry Pi regarding their accuracy, processing memory, non-volatile memory, and total prediction time. Finally, the best classifier was deployed on the Raspberry Pi to evaluate the ESR algorithm's performance in the realistic context of a passenger vehicle, taking into consideration factors such as background noise, varying environmental conditions, and real-time response given road conditions.


% Specific objective a) 

The three benchmark datasets exhibit notable distinctions, despite originating from the same source repository (Freesound), they differ significantly in terms of sample duration and sound quality. ESC-10 and BDLib2 showcase cleaner signals predominantly focused on foreground sounds, whereas BDLib2 lacks sample representativeness. Conversely, \gls{us8k} presents a mixture of background and foreground sounds and boasts a much larger sample size compared to both ESC-10 and BDLib2. One could posit a hypothesis that ESC-10 would achieve the highest accuracy rates, followed by BDLib2 and \gls{us8k}, which was in line with some findings in the literature, either using classical machine learning techniques such as \textcite{Silva2019} and \textcite{Bountourakis2019} or \gls{cnn} 1D such as \textcite{Vandendriessche2021}, notably, these authors followed the dataset specifications regarding k-fold cross-validation strictly. Others like \textcite{Lhoest2021} and \textcite{Luz2021} achieved remarkably high accuracy rates for \gls{us8k} using Random Forest and \gls{k-nn}, respectively 94,2\% and 93,2\%, however, they did not comply with the dataset specifications. For comparison, these results surpass the official highest score for \gls{us8k} obtained with deep neural networks, which stood at 90\% in 2022. The results presented in Table \ref{table:results_accuracy_comparison} using the proposed \gls{esr} algorithm either overcame or matched the literature in terms of accuracy rates for machine learning techniques, ensemble methods, and simple neural networks.

%Specific objective b) and c) 

During the training and classification flow, the normalization process yielded better results than standardization. The \gls{pca} technique, while producing similar accuracy results, presented a low trade-off between processing memory and total prediction time. The augmentation process increased accuracy on the ESC-10 and BDLib2 datasets but decreased it by 0.70\% on the \gls{us8k} dataset. Additionally, the windowed models (more relevant for full prediction time) showed statistically significant lower accuracy results (5\%) across classifiers ($p$-value < 0.05) and datasets ($p$-value < 0.005). Following, a \gls{cnn} 2D using pseudo-images of aggregated features from log-mel spectrograms, along with their delta and delta-delta, was included among the top-performing classifiers (\gls{svm}, \gls{lr}, \gls{rf}, \gls{ann}, and \gls{cnn} 1D) that used handcrafted features.

After creating a new dataset, US8K\_AV \cite{florentino2024}, comprising 4,908 files distributed among 6 classes in a 10-fold split for cross-validation, with a total duration of 4.94 hours of recorded sounds and a size of 3.27 GB, a comprehensive baseline for the C-Bot was established. The top-performing classifiers were then cross-validated (10-fold) on this dataset, producing the accuracy results shown in Table \ref{table:results_feature_extraction_and_classifiers_us8k_av}, with an average accuracy of 83\% in the single window process, except for \gls{rf} with 80\%. Notably, in the sliding window process, \gls{cnn} 2D achieved the highest accuracy (80\%) among all analyzed classifiers.

The predictive models using sliding window (1 \gls{s}) were trained using folds 2 through 10, reserving fold 1 as the testing set. Anticipating the embedding process, the predictive models for \gls{ann}, \gls{cnn} 1D, and \gls{cnn} 2D underwent a quantization process using TensorFlow Lite (TFLite), with no significant decline in accuracy following this process. The final stage evaluated the predictive algorithms regarding total prediction time, processing memory (RAM), and non-volatile memory (Flash). \gls{cnn} 2D had the lowest prediction time on both the notebook (30.2 \gls{mi}\gls{s}) and Raspberry Pi (47.6 \gls{mi}\gls{s}), while \gls{lr} had the second-lowest RAM usage on the Raspberry Pi (1.1 \gls{m}\gls{b}), the second lowest total prediction time in both devices and the lowest Flash usage (19.0 \gls{k}\gls{b}). These findings, together with the spider web chart (Figure \ref{fig:Results_spider_chart_us8k_av_cnn2d}), supported the decision to deploy the \gls{esr} algorithm on the Raspberry Pi with the \textbf{predictive model \gls{cnn} 2D}. Remarkably, this decision was primarily influenced by the total prediction time, which is longer for feature extraction in the handcrafted approach compared to the aggregated approach, rather than by the classification time itself.

Prior to the evaluation flow, an indoor experiment was conducted with the Raspberry Pi and the \gls{esr} algorithm using \gls{cnn} 2D. Given the average accuracy of 80\% obtained in the cross-validation, the results presented in Table \ref{table:results_indoor_experiments} indicate a sound algorithm with a weighted F1-score near this value for dog\_bark (77\%) and higher for children\_playing (92\%), car\_horn (89\%), and siren (95\%).


%Specific objective d) 

Finally, during the evaluation phase using the passenger vehicle VW UP, the experiment "Driving on streets 01" confirmed that the \gls{esr} algorithm was robust, producing a weighted F1-score of 99\% during approximately 1 hour of continuous driving in the streets of Santo André and São Bernardo do Campo. The subsequent three experiments demonstrated that higher chunk sizes are needed on the Raspberry Pi to avoid distortions in the audio clip, which resulted in incorrect inferences. A chunk size of 8,192 samples led to an audio clip duration of 1.114578 \gls{s}, which ultimately resulted in a full prediction time of 1.162178 \gls{s} (47.6 \gls{mi}\gls{s} for total prediction time + 1.114578 \gls{s} for audio digitization). There is no basis for comparison in the literature for this result. Notably, if factors such as RAM and Flash were more relevant than the total prediction time, the \gls{esr} algorithm with the predictive model \gls{lr} would yield similar F1-scores while demanding fewer resources.

The results of the next set of experiments ("Driving on streets 05 to 08"), compiled in Table \ref{table:results_outdoor_experiments}, although very close to the expected average accuracy of 80\%, showed some inconsistencies with the indoor experiment, such as the class car horn with F1-score of 74\% (precision of 100\% and recall of 59\%), and the class siren with F1-score of 73\% (precision of 65\% and recall of 85\%). Upon extensive examination of the false positives and negatives of these two classes and the trained samples, it may be inferred that the class car\_horn lacks short-period samples (less than 0.2 \gls{s}). Additionally, most of the false positives in the class siren were caused by high-frequency sounds produced by either worn brake pads or faster vehicles passing by.

Overall, this project contributes to a better understanding of the \gls{esr} algorithm in embedded systems within the context of a passenger vehicle, demonstrating that environmental sounds other than emergency vehicles can be detected. It entails that the project successfully achieved its general objective and is ready for the next steps of integration into the \gls{ee} architecture of the C-Bot. To facilitate this, a comprehensive step-by-step guide has been made publicly available in Appendix \ref{appendix:source_code}.


\section{FUTURE WORK}
\label{sec:results_future_work}

As a natural progression of this work, the lapel microphone should be replaced with the C-Bot external microphone (ReSpeaker Mic Array v2.0). The class 'car\_horn' should be populated with more diverse samples to better balance the dataset, especially with short-period samples. Additionally, a new class named 'road\_sounds' could be added to improve the accuracy of relevant classes and reduce false positives and negatives. The class 'bike\_horn' should also be considered based on the context of the C-Bot.

To reduce the full prediction time, a shorter sliding window (e.g., 0.5 \gls{s}) should be evaluated. Similarly, to increase the weighted F1-score and average accuracy, different \gls{cnn} 2D architectures can be proposed to balance the trade-off between total prediction time, processing memory, and non-volatile memory.

Lastly, further experiments using other passenger vehicles should be conducted to assess the influence of cabin insulation across different vehicle categories. If differences are marginal, this project could be adapted as an add-on feature in passenger vehicles, enhancing overall safety for drivers with hearing impairments.